{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Neural networks - MLP and DNN\n",
    "\n",
    "Multi-layer Perceptron (MLP) is a supervised learning algorithm that learns a function by training on a dataset. It is a universal approximator of any function. General work principles rely on feedforward signal propagation to predict and backward error propagation to adjust weights and adapt to problems/data.\n",
    "\n",
    "![Perceptron.png](./img/Perceptron.png)\n",
    "\n",
    "Source: https://pl.wikipedia.org/wiki/Perceptron#/media/Plik:Perceptron_moj.png\n",
    "\n",
    "![ANN2.png](./img/ANN2.png)\n",
    "\n",
    "Source: https://en.wikipedia.org/wiki/Neural_network\n",
    "\n",
    "A deep neural network (DNN) is an artificial neural network (ANN) with more complex architecture within a number of layers. The DNN during a learning process adapts to data and gathered knowledge is stored in weights conecting artificial neurons. Development and learning of a very complex neural network require a specific approach called deep learning which is related to technics preventing overfitting as well as reduce the time required to learn the model. DNN could have some specilized layers introduced like filters responsible for data convolution (Convolutional Neural Networks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Problem - predict if given chemical compound is agonist of estrogen nuclear receptor alpha"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2014 Tox21 data challenge was designed to develop models capable to predict the affinity of chemical compounds to specific receptors. General idea was to develop a tools with possible applications in searching for new active compounds and perform screening in search of new potential drugs. We will focus on the assessment of compound affinity to estrogen alpha receptors. \n",
    "\n",
    "qHTS assay to identify small molecule agonists of the estrogen receptor alpha (ER-alpha) signaling pathway using the BG1 cell line\n",
    "\n",
    "Tox21 challange: https://tripod.nih.gov/tox21/challenge/index.jsp\n",
    "\n",
    "Dataset we will explore: https://pubchem.ncbi.nlm.nih.gov/bioassay/743079\n",
    "\n",
    "![data.png](/img/data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import all required packages\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn import linear_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.regularizers import *\n",
    "from keras.wrappers.scikit_learn import *\n",
    "#Load the data\n",
    "df_raw = pd.read_csv(\"estrogen_nuclear_receptor.txt\", sep=\"\\t\")\n",
    "\n",
    "#Function to calculate fingerprints with specified length of binary outcome\n",
    "def morgan_fp(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, 3, nBits=10000)\n",
    "    npfp=np.array(list(fp.ToBitString())).astype('float').T\n",
    "    return npfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate fingerprints\n",
    "data=np.array([np.arange(df_raw.shape[0])]*10000).T\n",
    "for i, row in df_raw.iterrows():\n",
    "    data[i]=morgan_fp(df_raw[\"SMILES\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8472, 10000)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create input and output vectors of variables\n",
    "X=data\n",
    "y=pd.DataFrame(df_raw['Agonist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7624/7624 [==============================] - 3s 407us/step - loss: 35.2508 - accuracy: 0.8804\n",
      "Epoch 2/10\n",
      "7624/7624 [==============================] - 2s 226us/step - loss: 27.4642 - accuracy: 0.8804\n",
      "Epoch 3/10\n",
      "7624/7624 [==============================] - 1s 184us/step - loss: 21.1134 - accuracy: 0.8804\n",
      "Epoch 4/10\n",
      "7624/7624 [==============================] - 1s 156us/step - loss: 16.1704 - accuracy: 0.8804\n",
      "Epoch 5/10\n",
      "7624/7624 [==============================] - 1s 166us/step - loss: 12.4149 - accuracy: 0.8804\n",
      "Epoch 6/10\n",
      "7624/7624 [==============================] - 1s 174us/step - loss: 9.5650 - accuracy: 0.8804\n",
      "Epoch 7/10\n",
      "7624/7624 [==============================] - 1s 169us/step - loss: 7.4035 - accuracy: 0.8804\n",
      "Epoch 8/10\n",
      "7624/7624 [==============================] - 1s 177us/step - loss: 5.7670 - accuracy: 0.8804\n",
      "Epoch 9/10\n",
      "7624/7624 [==============================] - 1s 184us/step - loss: 4.5229 - accuracy: 0.8804\n",
      "Epoch 10/10\n",
      "7624/7624 [==============================] - 1s 162us/step - loss: 3.5761 - accuracy: 0.8804\n",
      "848/848 [==============================] - 0s 267us/step\n",
      "848/848 [==============================] - 0s 71us/step\n",
      "7624/7624 [==============================] - 1s 117us/step\n",
      "7624/7624 [==============================] - 1s 110us/step\n",
      "Epoch 1/10\n",
      "7624/7624 [==============================] - 2s 325us/step - loss: 34.7716 - accuracy: 0.8804\n",
      "Epoch 2/10\n",
      "7624/7624 [==============================] - 1s 192us/step - loss: 26.9951 - accuracy: 0.8804\n",
      "Epoch 3/10\n",
      "7624/7624 [==============================] - 1s 175us/step - loss: 20.7695 - accuracy: 0.8804\n",
      "Epoch 4/10\n",
      "7624/7624 [==============================] - 1s 163us/step - loss: 15.9147 - accuracy: 0.8804\n",
      "Epoch 5/10\n",
      "7624/7624 [==============================] - 1s 156us/step - loss: 12.2097 - accuracy: 0.8804\n",
      "Epoch 6/10\n",
      "7624/7624 [==============================] - 1s 162us/step - loss: 9.4012 - accuracy: 0.8804\n",
      "Epoch 7/10\n",
      "7624/7624 [==============================] - 1s 167us/step - loss: 7.2790 - accuracy: 0.8804\n",
      "Epoch 8/10\n",
      "7624/7624 [==============================] - 1s 150us/step - loss: 5.6679 - accuracy: 0.8804\n",
      "Epoch 9/10\n",
      "7624/7624 [==============================] - 1s 163us/step - loss: 4.4465 - accuracy: 0.8804\n",
      "Epoch 10/10\n",
      "7624/7624 [==============================] - 1s 174us/step - loss: 3.5124 - accuracy: 0.8804\n",
      "848/848 [==============================] - 0s 222us/step\n",
      "848/848 [==============================] - 0s 86us/step\n",
      "7624/7624 [==============================] - 1s 108us/step\n",
      "7624/7624 [==============================] - 1s 107us/step\n",
      "Epoch 1/10\n",
      "7625/7625 [==============================] - 3s 339us/step - loss: 34.5978 - accuracy: 0.8803\n",
      "Epoch 2/10\n",
      "7625/7625 [==============================] - 1s 166us/step - loss: 26.8774 - accuracy: 0.8803\n",
      "Epoch 3/10\n",
      "7625/7625 [==============================] - 1s 168us/step - loss: 20.7825 - accuracy: 0.8803\n",
      "Epoch 4/10\n",
      "7625/7625 [==============================] - 1s 148us/step - loss: 16.0135 - accuracy: 0.8803\n",
      "Epoch 5/10\n",
      "7625/7625 [==============================] - 1s 152us/step - loss: 12.3443 - accuracy: 0.8803\n",
      "Epoch 6/10\n",
      "7625/7625 [==============================] - 1s 164us/step - loss: 9.5380 - accuracy: 0.8803\n",
      "Epoch 7/10\n",
      "7625/7625 [==============================] - 1s 160us/step - loss: 7.3985 - accuracy: 0.8803\n",
      "Epoch 8/10\n",
      "7625/7625 [==============================] - 1s 180us/step - loss: 5.7668 - accuracy: 0.8803\n",
      "Epoch 9/10\n",
      "7625/7625 [==============================] - 1s 173us/step - loss: 4.5217 - accuracy: 0.8803\n",
      "Epoch 10/10\n",
      "7625/7625 [==============================] - 1s 170us/step - loss: 3.5681 - accuracy: 0.8803\n",
      "847/847 [==============================] - 0s 211us/step\n",
      "847/847 [==============================] - 0s 73us/step\n",
      "7625/7625 [==============================] - 1s 109us/step\n",
      "7625/7625 [==============================] - 1s 121us/step\n",
      "Epoch 1/10\n",
      "7625/7625 [==============================] - 2s 303us/step - loss: 34.6531 - accuracy: 0.8803\n",
      "Epoch 2/10\n",
      "7625/7625 [==============================] - 1s 175us/step - loss: 26.8958 - accuracy: 0.8803\n",
      "Epoch 3/10\n",
      "7625/7625 [==============================] - 1s 163us/step - loss: 20.7956 - accuracy: 0.8803\n",
      "Epoch 4/10\n",
      "7625/7625 [==============================] - 1s 171us/step - loss: 15.9987 - accuracy: 0.8803\n",
      "Epoch 5/10\n",
      "7625/7625 [==============================] - 1s 179us/step - loss: 12.3019 - accuracy: 0.8803\n",
      "Epoch 6/10\n",
      "7625/7625 [==============================] - 1s 173us/step - loss: 9.4760 - accuracy: 0.8803\n",
      "Epoch 7/10\n",
      "7625/7625 [==============================] - 1s 188us/step - loss: 7.3256 - accuracy: 0.8803\n",
      "Epoch 8/10\n",
      "7625/7625 [==============================] - 1s 161us/step - loss: 5.6922 - accuracy: 0.8803\n",
      "Epoch 9/10\n",
      "7625/7625 [==============================] - 1s 176us/step - loss: 4.4505 - accuracy: 0.8803\n",
      "Epoch 10/10\n",
      "7625/7625 [==============================] - 2s 233us/step - loss: 3.5030 - accuracy: 0.8803\n",
      "847/847 [==============================] - 0s 186us/step\n",
      "847/847 [==============================] - 0s 86us/step\n",
      "7625/7625 [==============================] - 1s 109us/step\n",
      "7625/7625 [==============================] - 1s 90us/step\n",
      "Epoch 1/10\n",
      "7625/7625 [==============================] - 2s 309us/step - loss: 35.1786 - accuracy: 0.8803\n",
      "Epoch 2/10\n",
      "7625/7625 [==============================] - 1s 172us/step - loss: 27.3243 - accuracy: 0.8803\n",
      "Epoch 3/10\n",
      "7625/7625 [==============================] - 1s 169us/step - loss: 21.0276 - accuracy: 0.8803\n",
      "Epoch 4/10\n",
      "7625/7625 [==============================] - 1s 168us/step - loss: 16.1035 - accuracy: 0.8803\n",
      "Epoch 5/10\n",
      "7625/7625 [==============================] - 1s 167us/step - loss: 12.3415 - accuracy: 0.8803\n",
      "Epoch 6/10\n",
      "7625/7625 [==============================] - 1s 183us/step - loss: 9.4888 - accuracy: 0.8803\n",
      "Epoch 7/10\n",
      "7625/7625 [==============================] - 1s 181us/step - loss: 7.3282 - accuracy: 0.8803\n",
      "Epoch 8/10\n",
      "7625/7625 [==============================] - 1s 175us/step - loss: 5.6919 - accuracy: 0.8803\n",
      "Epoch 9/10\n",
      "7625/7625 [==============================] - 1s 169us/step - loss: 4.4519 - accuracy: 0.8803\n",
      "Epoch 10/10\n",
      "7625/7625 [==============================] - 1s 190us/step - loss: 3.5067 - accuracy: 0.8803\n",
      "847/847 [==============================] - 0s 265us/step\n",
      "847/847 [==============================] - 0s 140us/step\n",
      "7625/7625 [==============================] - 1s 126us/step\n",
      "7625/7625 [==============================] - 1s 122us/step\n",
      "Epoch 1/10\n",
      "7625/7625 [==============================] - 2s 299us/step - loss: 35.2535 - accuracy: 0.8803\n",
      "Epoch 2/10\n",
      "7625/7625 [==============================] - 2s 211us/step - loss: 27.5062 - accuracy: 0.8803\n",
      "Epoch 3/10\n",
      "7625/7625 [==============================] - 1s 182us/step - loss: 21.1697 - accuracy: 0.8803\n",
      "Epoch 4/10\n",
      "7625/7625 [==============================] - 1s 154us/step - loss: 16.2365 - accuracy: 0.8803\n",
      "Epoch 5/10\n",
      "7625/7625 [==============================] - 1s 169us/step - loss: 12.4631 - accuracy: 0.8803\n",
      "Epoch 6/10\n",
      "7625/7625 [==============================] - 1s 186us/step - loss: 9.5968 - accuracy: 0.8803\n",
      "Epoch 7/10\n",
      "7625/7625 [==============================] - 1s 185us/step - loss: 7.4258 - accuracy: 0.8803\n",
      "Epoch 8/10\n",
      "7625/7625 [==============================] - 1s 173us/step - loss: 5.7787 - accuracy: 0.8803\n",
      "Epoch 9/10\n",
      "7625/7625 [==============================] - 1s 150us/step - loss: 4.5280 - accuracy: 0.8803\n",
      "Epoch 10/10\n",
      "7625/7625 [==============================] - 1s 160us/step - loss: 3.5735 - accuracy: 0.8803\n",
      "847/847 [==============================] - 0s 248us/step\n",
      "847/847 [==============================] - 0s 66us/step\n",
      "7625/7625 [==============================] - 1s 99us/step\n",
      "7625/7625 [==============================] - 1s 87us/step\n",
      "Epoch 1/10\n",
      "7625/7625 [==============================] - 3s 353us/step - loss: 34.4591 - accuracy: 0.8803\n",
      "Epoch 2/10\n",
      "7625/7625 [==============================] - 1s 192us/step - loss: 26.5904 - accuracy: 0.8803\n",
      "Epoch 3/10\n",
      "7625/7625 [==============================] - 1s 182us/step - loss: 20.4129 - accuracy: 0.8803\n",
      "Epoch 4/10\n",
      "7625/7625 [==============================] - 1s 170us/step - loss: 15.6397 - accuracy: 0.8803\n",
      "Epoch 5/10\n",
      "7625/7625 [==============================] - 1s 175us/step - loss: 11.9909 - accuracy: 0.8803\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7625/7625 [==============================] - 1s 153us/step - loss: 9.2178 - accuracy: 0.8803\n",
      "Epoch 7/10\n",
      "7625/7625 [==============================] - 1s 150us/step - loss: 7.1171 - accuracy: 0.8803\n",
      "Epoch 8/10\n",
      "7625/7625 [==============================] - 1s 158us/step - loss: 5.5246 - accuracy: 0.8803\n",
      "Epoch 9/10\n",
      "7625/7625 [==============================] - 1s 163us/step - loss: 4.3152 - accuracy: 0.8803\n",
      "Epoch 10/10\n",
      "7625/7625 [==============================] - 1s 161us/step - loss: 3.3937 - accuracy: 0.8803\n",
      "847/847 [==============================] - 0s 322us/step\n",
      "847/847 [==============================] - 0s 40us/step\n",
      "7625/7625 [==============================] - 1s 113us/step\n",
      "7625/7625 [==============================] - 1s 108us/step\n",
      "Epoch 1/10\n",
      "7625/7625 [==============================] - 6s 784us/step - loss: 34.1643 - accuracy: 0.8803\n",
      "Epoch 2/10\n",
      "7625/7625 [==============================] - 1s 180us/step - loss: 26.5607 - accuracy: 0.8803\n",
      "Epoch 3/10\n",
      "7625/7625 [==============================] - 1s 176us/step - loss: 20.4305 - accuracy: 0.8803\n",
      "Epoch 4/10\n",
      "7625/7625 [==============================] - 1s 159us/step - loss: 15.6460 - accuracy: 0.8803\n",
      "Epoch 5/10\n",
      "7625/7625 [==============================] - 1s 162us/step - loss: 11.9889 - accuracy: 0.8803\n",
      "Epoch 6/10\n",
      "7625/7625 [==============================] - 1s 158us/step - loss: 9.2084 - accuracy: 0.8803\n",
      "Epoch 7/10\n",
      "7625/7625 [==============================] - 1s 168us/step - loss: 7.1030 - accuracy: 0.8803\n",
      "Epoch 8/10\n",
      "7625/7625 [==============================] - 1s 174us/step - loss: 5.5162 - accuracy: 0.8803\n",
      "Epoch 9/10\n",
      "7625/7625 [==============================] - 1s 162us/step - loss: 4.3147 - accuracy: 0.8803\n",
      "Epoch 10/10\n",
      "7625/7625 [==============================] - 1s 132us/step - loss: 3.4001 - accuracy: 0.8803\n",
      "847/847 [==============================] - 0s 580us/step\n",
      "847/847 [==============================] - 0s 40us/step\n",
      "7625/7625 [==============================] - 1s 102us/step\n",
      "7625/7625 [==============================] - 1s 94us/step\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "#Define model architecture and perform 10-cv calculations\n",
    "def DNN_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(100, kernel_initializer='normal', activation='relu',    kernel_regularizer=l1_l2(l1=0.0, l2=0.5)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(100, kernel_initializer='normal', activation='relu', kernel_regularizer=l1_l2(l1=0.0, l2=0.5)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(100, kernel_initializer='normal', activation='relu', kernel_regularizer=l1_l2(l1=0.0, l2=0.5)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])    \n",
    "    return model\n",
    "\n",
    "dnn = KerasClassifier(build_fn=DNN_model, epochs=10, batch_size=1000, verbose=1)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=123)\n",
    "results = cross_validate(dnn, \n",
    "                        X_train, \n",
    "                        y_train, \n",
    "                        cv=kfold, \n",
    "                        scoring=['accuracy', 'balanced_accuracy', 'roc_auc', 'f1'], \n",
    "                        return_train_score=True, \n",
    "                        return_estimator=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([118.74217892, 111.05812097, 118.52625012, 127.69321537,\n",
       "        117.79111862, 120.28922796, 122.0893321 , 126.15829253,\n",
       "        125.78135157, 133.53012562]),\n",
       " 'score_time': array([0.46792293, 0.37630415, 2.74190092, 0.41623378, 0.35397983,\n",
       "        2.65362167, 2.59019184, 1.95253706, 2.77430415, 3.10672784]),\n",
       " 'estimator': (<keras.wrappers.scikit_learn.KerasClassifier at 0x7f89850fe5c0>,\n",
       "  <keras.wrappers.scikit_learn.KerasClassifier at 0x7f89bc4e8208>,\n",
       "  <keras.wrappers.scikit_learn.KerasClassifier at 0x7f89c4557550>,\n",
       "  <keras.wrappers.scikit_learn.KerasClassifier at 0x7f89bda88860>,\n",
       "  <keras.wrappers.scikit_learn.KerasClassifier at 0x7f89880a1198>,\n",
       "  <keras.wrappers.scikit_learn.KerasClassifier at 0x7f89837fc208>,\n",
       "  <keras.wrappers.scikit_learn.KerasClassifier at 0x7f8981ecd0b8>,\n",
       "  <keras.wrappers.scikit_learn.KerasClassifier at 0x7f89804fc4e0>,\n",
       "  <keras.wrappers.scikit_learn.KerasClassifier at 0x7f897eb04a20>,\n",
       "  <keras.wrappers.scikit_learn.KerasClassifier at 0x7f893b07b978>),\n",
       " 'test_accuracy': array([0.87971698, 0.87971698, 0.88075561, 0.88075561, 0.88075561,\n",
       "        0.88075561, 0.88075561, 0.88075561, 0.87957497, 0.87957497]),\n",
       " 'train_accuracy': array([0.88037775, 0.88037775, 0.8802623 , 0.8802623 , 0.8802623 ,\n",
       "        0.8802623 , 0.8802623 , 0.8802623 , 0.88039344, 0.88039344]),\n",
       " 'test_balanced_accuracy': array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]),\n",
       " 'train_balanced_accuracy': array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]),\n",
       " 'test_roc_auc': array([0.54076644, 0.46614624, 0.46180288, 0.53656465, 0.52872747,\n",
       "        0.50326494, 0.42912696, 0.51037215, 0.51809449, 0.51038952]),\n",
       " 'train_roc_auc': array([0.54803618, 0.40054578, 0.45777478, 0.60821874, 0.5300597 ,\n",
       "        0.5094521 , 0.47142977, 0.5215827 , 0.47415928, 0.47756236]),\n",
       " 'test_f1': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'train_f1': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['test_roc_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adam/anaconda3/envs/regression/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#Compare with a Linear model\n",
    "fitscores = []\n",
    "alphas = np.logspace(-2, 4, num=10)\n",
    "for alpha in alphas:\n",
    "    estimator = linear_model.LogisticRegression(C = 1/alpha)\n",
    "      \n",
    "\n",
    "    results = cross_validate(estimator, \n",
    "                                         X_train, \n",
    "                                         y_train, \n",
    "                                         cv=kfold, \n",
    "                                         scoring=['accuracy', 'roc_auc', 'f1'], \n",
    "                                         return_train_score=True, \n",
    "                                         return_estimator=True)\n",
    "    \n",
    "    fitscores.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'fit_time': array([7.48042917, 6.38767385, 6.08726859, 6.08746243, 6.3068459 ,\n",
       "         6.13200331, 6.33558989, 6.00037503, 6.14705253, 5.99856544]),\n",
       "  'score_time': array([0.09256196, 0.03329754, 0.03242826, 0.03232718, 0.0276804 ,\n",
       "         0.03450823, 0.03423858, 0.0318563 , 0.03415322, 0.03790474]),\n",
       "  'estimator': (LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False)),\n",
       "  'test_accuracy': array([0.88441558, 0.85844156, 0.87256177, 0.88816645, 0.88556567,\n",
       "         0.88816645, 0.87516255, 0.87386216, 0.88296489, 0.8946684 ]),\n",
       "  'train_accuracy': array([0.9790523 , 0.9794857 , 0.98035534, 0.979922  , 0.97746642,\n",
       "         0.97905532, 0.97977755, 0.97934421, 0.97905532, 0.98006645]),\n",
       "  'test_roc_auc': array([0.69830354, 0.66741943, 0.73497646, 0.72663358, 0.67688649,\n",
       "         0.68893555, 0.75081166, 0.73487786, 0.67902285, 0.75424744]),\n",
       "  'train_roc_auc': array([0.99763066, 0.99769073, 0.99784419, 0.99780614, 0.99725702,\n",
       "         0.99770595, 0.99780038, 0.99761582, 0.99764472, 0.99780175]),\n",
       "  'test_f1': array([0.46706587, 0.34730539, 0.48421053, 0.48809524, 0.45      ,\n",
       "         0.47560976, 0.43529412, 0.43930636, 0.4375    , 0.53714286]),\n",
       "  'train_f1': array([0.90897677, 0.91256158, 0.91604938, 0.91456669, 0.90298507,\n",
       "         0.91120637, 0.91347342, 0.91178285, 0.91109749, 0.91491985])},\n",
       " {'fit_time': array([5.89819622, 6.18262029, 6.28833985, 6.19653702, 5.98975062,\n",
       "         5.97557664, 5.99558759, 5.974226  , 6.05461884, 6.15347242]),\n",
       "  'score_time': array([0.03500867, 0.03514242, 0.04504418, 0.03468657, 0.03514385,\n",
       "         0.03366852, 0.03448415, 0.0363946 , 0.04280806, 0.04192615]),\n",
       "  'estimator': (LogisticRegression(C=21.544346900318843, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=21.544346900318843, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=21.544346900318843, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=21.544346900318843, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=21.544346900318843, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=21.544346900318843, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=21.544346900318843, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=21.544346900318843, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=21.544346900318843, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=21.544346900318843, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False)),\n",
       "  'test_accuracy': array([0.88961039, 0.87142857, 0.87516255, 0.8816645 , 0.89596879,\n",
       "         0.88036411, 0.87516255, 0.87776333, 0.88296489, 0.89336801]),\n",
       "  'train_accuracy': array([0.9786189 , 0.97847443, 0.97948866, 0.97934421, 0.97732197,\n",
       "         0.9780442 , 0.97934421, 0.97818865, 0.97891088, 0.97963311]),\n",
       "  'test_roc_auc': array([0.71314365, 0.68620956, 0.73957339, 0.74817077, 0.68134027,\n",
       "         0.6986066 , 0.74888889, 0.74483846, 0.68710796, 0.75449961]),\n",
       "  'train_roc_auc': array([0.99752127, 0.99751453, 0.9976842 , 0.99765786, 0.99716278,\n",
       "         0.99750324, 0.99765136, 0.99743184, 0.99755488, 0.99770976]),\n",
       "  'test_f1': array([0.47204969, 0.38509317, 0.48387097, 0.47398844, 0.48051948,\n",
       "         0.44578313, 0.44186047, 0.44047619, 0.43037975, 0.53409091]),\n",
       "  'train_f1': array([0.90632911, 0.90563648, 0.91046658, 0.91123526, 0.89980855,\n",
       "         0.90367554, 0.91034483, 0.90473186, 0.90782828, 0.91013384])},\n",
       " {'fit_time': array([6.06750941, 6.37879968, 5.82200074, 5.96737409, 6.43855619,\n",
       "         6.31431341, 6.07131481, 6.07439828, 5.98472309, 6.13926387]),\n",
       "  'score_time': array([0.03495646, 0.03587079, 0.03544807, 0.03524876, 0.0347898 ,\n",
       "         0.04204845, 0.03497005, 0.03544188, 0.03303409, 0.03513217]),\n",
       "  'estimator': (LogisticRegression(C=4.641588833612779, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=4.641588833612779, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=4.641588833612779, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=4.641588833612779, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=4.641588833612779, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=4.641588833612779, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=4.641588833612779, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=4.641588833612779, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=4.641588833612779, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=4.641588833612779, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False)),\n",
       "  'test_accuracy': array([0.8974026 , 0.88051948, 0.88426528, 0.89336801, 0.89986996,\n",
       "         0.89336801, 0.88556567, 0.88686606, 0.88816645, 0.89856957]),\n",
       "  'train_accuracy': array([0.97616296, 0.97630743, 0.97674419, 0.97703308, 0.97472194,\n",
       "         0.97573306, 0.97659974, 0.97573306, 0.97732197, 0.97659974]),\n",
       "  'test_roc_auc': array([0.71117651, 0.68885339, 0.74493383, 0.76218426, 0.69206114,\n",
       "         0.70376026, 0.75776202, 0.75613869, 0.6950197 , 0.75257683]),\n",
       "  'train_roc_auc': array([0.99695714, 0.99686113, 0.9970185 , 0.9970025 , 0.99651913,\n",
       "         0.99692026, 0.99693713, 0.9968014 , 0.99700764, 0.99707385]),\n",
       "  'test_f1': array([0.48366013, 0.41772152, 0.5027933 , 0.5       , 0.49673203,\n",
       "         0.47435897, 0.45      , 0.46625767, 0.44871795, 0.53571429]),\n",
       "  'train_f1': array([0.89334195, 0.89460154, 0.89672867, 0.89788054, 0.88614183,\n",
       "         0.89133247, 0.89534884, 0.89076723, 0.89877498, 0.89507772])},\n",
       " {'fit_time': array([6.00767493, 6.05220556, 6.13751149, 6.11278057, 6.19413519,\n",
       "         6.01637816, 6.02623129, 5.93340516, 6.0850594 , 5.75834322]),\n",
       "  'score_time': array([0.03431606, 0.03596616, 0.03535557, 0.03520298, 0.03462839,\n",
       "         0.03624606, 0.03468442, 0.03502631, 0.03456593, 0.03458095]),\n",
       "  'estimator': (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False)),\n",
       "  'test_accuracy': array([0.9       , 0.89480519, 0.89336801, 0.91157347, 0.90377113,\n",
       "         0.89726918, 0.8946684 , 0.89986996, 0.89986996, 0.91157347]),\n",
       "  'train_accuracy': array([0.96763941, 0.96836175, 0.96836632, 0.96822187, 0.96648852,\n",
       "         0.96836632, 0.96735519, 0.96663296, 0.96721075, 0.96663296]),\n",
       "  'test_roc_auc': array([0.71895065, 0.69952317, 0.75492301, 0.76784692, 0.71439365,\n",
       "         0.7154037 , 0.77038613, 0.77268716, 0.70526399, 0.75768322]),\n",
       "  'train_roc_auc': array([0.99227648, 0.99202576, 0.99196801, 0.99213931, 0.99207697,\n",
       "         0.99229666, 0.99215978, 0.99176859, 0.99264258, 0.9922754 ]),\n",
       "  'test_f1': array([0.47619048, 0.44137931, 0.49382716, 0.55263158, 0.50666667,\n",
       "         0.46979866, 0.44897959, 0.47619048, 0.46896552, 0.54666667]),\n",
       "  'train_f1': array([0.8490566 , 0.8527236 , 0.85252525, 0.85195155, 0.84260516,\n",
       "         0.8527236 , 0.8472973 , 0.84296397, 0.84610169, 0.84338983])},\n",
       " {'fit_time': array([4.36851072, 4.38396215, 4.43242025, 3.9477241 , 4.54435253,\n",
       "         4.36406755, 4.22489476, 4.326226  , 4.61588478, 4.37325764]),\n",
       "  'score_time': array([0.03476453, 0.03359914, 0.03552413, 0.03562713, 0.03547788,\n",
       "         0.03505015, 0.03240037, 0.03429484, 0.0349822 , 0.03623486]),\n",
       "  'estimator': (LogisticRegression(C=0.21544346900318845, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.21544346900318845, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.21544346900318845, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.21544346900318845, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.21544346900318845, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.21544346900318845, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.21544346900318845, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.21544346900318845, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.21544346900318845, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.21544346900318845, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False)),\n",
       "  'test_accuracy': array([0.91168831, 0.9       , 0.90247074, 0.91547464, 0.89986996,\n",
       "         0.89986996, 0.89596879, 0.90507152, 0.90637191, 0.91937581]),\n",
       "  'train_accuracy': array([0.94091303, 0.94163537, 0.94149935, 0.94149935, 0.94121046,\n",
       "         0.94236603, 0.94034378, 0.9413549 , 0.94149935, 0.94193269]),\n",
       "  'test_roc_auc': array([0.73149314, 0.7128053 , 0.7737084 , 0.77839282, 0.73518324,\n",
       "         0.72849462, 0.77336485, 0.78803783, 0.71547675, 0.76584712]),\n",
       "  'train_roc_auc': array([0.96715439, 0.96776335, 0.96646366, 0.96691923, 0.96761332,\n",
       "         0.96761819, 0.96759005, 0.96651997, 0.96819706, 0.96709729]),\n",
       "  'test_f1': array([0.46875   , 0.40310078, 0.46808511, 0.51851852, 0.4379562 ,\n",
       "         0.40310078, 0.36507937, 0.45925926, 0.44615385, 0.53731343]),\n",
       "  'train_f1': array([0.6846569 , 0.68923077, 0.68965517, 0.68917882, 0.6876439 ,\n",
       "         0.69518717, 0.68108108, 0.68721109, 0.688701  , 0.69218989])},\n",
       " {'fit_time': array([2.42421699, 2.58327675, 2.47606754, 2.5209868 , 2.60015726,\n",
       "         2.62474656, 2.34453511, 2.39387679, 2.42792535, 2.26191235]),\n",
       "  'score_time': array([0.03527236, 0.03546286, 0.03551531, 0.03336334, 0.03534245,\n",
       "         0.03554463, 0.03513908, 0.03561759, 0.03548694, 0.03582048]),\n",
       "  'estimator': (LogisticRegression(C=0.04641588833612782, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.04641588833612782, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.04641588833612782, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.04641588833612782, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.04641588833612782, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.04641588833612782, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.04641588833612782, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.04641588833612782, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.04641588833612782, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.04641588833612782, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False)),\n",
       "  'test_accuracy': array([0.9012987 , 0.9025974 , 0.91157347, 0.91547464, 0.90117035,\n",
       "         0.89856957, 0.89336801, 0.9076723 , 0.90377113, 0.91417425]),\n",
       "  'train_accuracy': array([0.91375325, 0.91433112, 0.91318792, 0.91203236, 0.91492128,\n",
       "         0.91419905, 0.91434349, 0.91347682, 0.91289903, 0.91232125]),\n",
       "  'test_roc_auc': array([0.7430914 , 0.72871554, 0.78703792, 0.79367882, 0.7508828 ,\n",
       "         0.73859515, 0.75899133, 0.78857368, 0.72187549, 0.77555556]),\n",
       "  'train_roc_auc': array([0.90321241, 0.90630469, 0.90247715, 0.90258817, 0.90420686,\n",
       "         0.90515644, 0.90439899, 0.90252328, 0.90462183, 0.90288772]),\n",
       "  'test_f1': array([0.3559322 , 0.35897436, 0.44262295, 0.46280992, 0.38709677,\n",
       "         0.36065574, 0.28070175, 0.41322314, 0.37288136, 0.49230769]),\n",
       "  'train_f1': array([0.46648794, 0.47100803, 0.4590459 , 0.44686649, 0.47363718,\n",
       "         0.4686941 , 0.47100803, 0.46084608, 0.45919283, 0.45067873])},\n",
       " {'fit_time': array([1.64134169, 1.58165669, 1.61966872, 1.61439848, 1.63469362,\n",
       "         1.5973525 , 1.65042424, 1.66129613, 1.63843799, 1.5861311 ]),\n",
       "  'score_time': array([0.03591275, 0.03592062, 0.03517795, 0.03603601, 0.03638935,\n",
       "         0.03633213, 0.03591537, 0.03484774, 0.03700614, 0.03531837]),\n",
       "  'estimator': (LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False)),\n",
       "  'test_accuracy': array([0.88961039, 0.89480519, 0.89726918, 0.89206762, 0.89206762,\n",
       "         0.89596879, 0.88816645, 0.8946684 , 0.8946684 , 0.90377113]),\n",
       "  'train_accuracy': array([0.89771742, 0.89800636, 0.89672107, 0.89758775, 0.89729886,\n",
       "         0.89787664, 0.89859887, 0.89715441, 0.89787664, 0.89643218]),\n",
       "  'test_roc_auc': array([0.73959776, 0.74275305, 0.77661927, 0.77977667, 0.76211268,\n",
       "         0.74745499, 0.73895981, 0.77246651, 0.71866036, 0.77624901]),\n",
       "  'train_roc_auc': array([0.82801161, 0.82986858, 0.82643666, 0.82581486, 0.82751354,\n",
       "         0.82902053, 0.82870061, 0.82574213, 0.83036949, 0.82638878]),\n",
       "  'test_f1': array([0.19047619, 0.24299065, 0.26168224, 0.19417476, 0.23853211,\n",
       "         0.27272727, 0.18867925, 0.27027027, 0.24299065, 0.36206897]),\n",
       "  'train_f1': array([0.2804878 , 0.28105906, 0.26666667, 0.27726809, 0.27522936,\n",
       "         0.28368794, 0.28803245, 0.27198364, 0.28077314, 0.26158599])},\n",
       " {'fit_time': array([1.20251966, 1.16263413, 1.21038032, 1.21759129, 1.22721529,\n",
       "         1.24626899, 1.18335485, 1.27460456, 1.22658801, 1.23837876]),\n",
       "  'score_time': array([0.03521299, 0.03863549, 0.0351243 , 0.03646398, 0.03537965,\n",
       "         0.03618002, 0.03569055, 0.03597188, 0.03618097, 0.03588033]),\n",
       "  'estimator': (LogisticRegression(C=0.0021544346900318864, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.0021544346900318864, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.0021544346900318864, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.0021544346900318864, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.0021544346900318864, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.0021544346900318864, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.0021544346900318864, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.0021544346900318864, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.0021544346900318864, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.0021544346900318864, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False)),\n",
       "  'test_accuracy': array([0.88311688, 0.87792208, 0.88296489, 0.8816645 , 0.87906372,\n",
       "         0.88296489, 0.87906372, 0.8816645 , 0.87906372, 0.88036411]),\n",
       "  'train_accuracy': array([0.88081479, 0.88139266, 0.88083201, 0.88097646, 0.88155424,\n",
       "         0.88083201, 0.88169869, 0.88097646, 0.88126535, 0.88083201]),\n",
       "  'test_roc_auc': array([0.72721264, 0.74365007, 0.75616371, 0.75553541, 0.75819972,\n",
       "         0.7464847 , 0.71801418, 0.75128448, 0.70162333, 0.76217494]),\n",
       "  'train_roc_auc': array([0.77930161, 0.77947868, 0.77790697, 0.77782386, 0.77823758,\n",
       "         0.78013107, 0.78029783, 0.7769753 , 0.78210811, 0.77726182]),\n",
       "  'test_f1': array([0.08163265, 0.        , 0.0625    , 0.04210526, 0.        ,\n",
       "         0.0625    , 0.02105263, 0.06185567, 0.02105263, 0.04166667]),\n",
       "  'train_f1': array([0.03958091, 0.04866744, 0.04181185, 0.04408353, 0.05311778,\n",
       "         0.04181185, 0.05317919, 0.04186047, 0.04640371, 0.03958091])},\n",
       " {'fit_time': array([1.00546241, 1.01475644, 1.00229716, 1.01763487, 1.03487778,\n",
       "         1.01830769, 1.00685978, 0.99869585, 1.01208949, 1.02447438]),\n",
       "  'score_time': array([0.03534698, 0.03645349, 0.03612375, 0.03566217, 0.04203272,\n",
       "         0.03655171, 0.03535724, 0.03677917, 0.03599572, 0.03748488]),\n",
       "  'estimator': (LogisticRegression(C=0.00046415888336127817, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.00046415888336127817, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.00046415888336127817, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.00046415888336127817, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.00046415888336127817, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.00046415888336127817, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.00046415888336127817, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.00046415888336127817, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.00046415888336127817, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.00046415888336127817, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False)),\n",
       "  'test_accuracy': array([0.87792208, 0.87792208, 0.87906372, 0.87906372, 0.87906372,\n",
       "         0.87906372, 0.87776333, 0.87776333, 0.87776333, 0.87776333]),\n",
       "  'train_accuracy': array([0.87835886, 0.87835886, 0.87823198, 0.87823198, 0.87823198,\n",
       "         0.87823198, 0.87837643, 0.87837643, 0.87837643, 0.87837643]),\n",
       "  'test_roc_auc': array([0.71906081, 0.73864566, 0.74440892, 0.74506903, 0.747622  ,\n",
       "         0.74250811, 0.7098818 , 0.7399212 , 0.69399527, 0.75527187]),\n",
       "  'train_roc_auc': array([0.75893227, 0.75809789, 0.75747399, 0.75715051, 0.75687492,\n",
       "         0.7591358 , 0.75855747, 0.75737744, 0.76117593, 0.75679094]),\n",
       "  'test_f1': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  'train_f1': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])},\n",
       " {'fit_time': array([1.06187105, 1.0721159 , 1.05074382, 1.07132339, 1.08380175,\n",
       "         1.05210876, 1.0624342 , 1.05502892, 1.06664419, 1.06001163]),\n",
       "  'score_time': array([0.03666735, 0.03575039, 0.03568292, 0.03642344, 0.03523302,\n",
       "         0.03720284, 0.03549886, 0.03596163, 0.03543425, 0.03531599]),\n",
       "  'estimator': (LogisticRegression(C=0.0001, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.0001, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.0001, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.0001, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.0001, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.0001, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.0001, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.0001, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.0001, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   LogisticRegression(C=0.0001, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False)),\n",
       "  'test_accuracy': array([0.87792208, 0.87792208, 0.87906372, 0.87906372, 0.87906372,\n",
       "         0.87906372, 0.87776333, 0.87776333, 0.87776333, 0.87776333]),\n",
       "  'train_accuracy': array([0.87835886, 0.87835886, 0.87823198, 0.87823198, 0.87823198,\n",
       "         0.87823198, 0.87837643, 0.87837643, 0.87837643, 0.87837643]),\n",
       "  'test_roc_auc': array([0.71496916, 0.73829945, 0.73981199, 0.74201502, 0.74382039,\n",
       "         0.74099701, 0.70854216, 0.73626478, 0.6920725 , 0.75249803]),\n",
       "  'train_roc_auc': array([0.7533972 , 0.75197251, 0.75152075, 0.7510722 , 0.75067487,\n",
       "         0.75288814, 0.75213175, 0.75179114, 0.75504179, 0.75092418]),\n",
       "  'test_f1': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  'train_f1': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitscores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions and suggestions are highly welcome! adam.paclawski@uj.edu.pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
